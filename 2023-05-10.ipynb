{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d930c11-94d2-410c-8500-aa683c2ccc66",
   "metadata": {},
   "source": [
    "#### 2023-05-10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53817a41-2805-46ec-b4fb-6339d37d7641",
   "metadata": {},
   "source": [
    "##### 1교시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c8741d-2af0-44d0-a8f0-7ecf7b9af3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 클렌징은 무엇인가요?\n",
    "텍스트 분석에 방해되는 불필요한 문자, 기호 등을 제거하는것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4aeac9-5ca2-4fe2-9547-cd048c7472e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어간 추출과 표제어 추출은 영어로 무엇인가요?\n",
    "stemming(어간) / lemmatization(표제어)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "419b85be-471a-4b55-a889-9f4fcb092f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/sonsoohan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문장 토큰화를 해 주세요\n",
    "text_sample = 'Turn your wounds into wisdom.\\\n",
    "               Life is either a daring adventure or nothing at all.\\\n",
    "               The two most important days in your life are the day you are born and the day you find out why.' \n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15da084e-5b5c-47e5-8868-48a4b1cf24b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Turn your wounds into wisdom.',\n",
       " 'Life is either a daring adventure or nothing at all.',\n",
       " 'The two most important days in your life are the day you are born and the day you find out why.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(text_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f768454-7adc-4684-9367-7fbc9faaf356",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Turn', 'your', 'wounds', 'into', 'wisdom', '.'],\n",
       " ['Life',\n",
       "  'is',\n",
       "  'either',\n",
       "  'a',\n",
       "  'daring',\n",
       "  'adventure',\n",
       "  'or',\n",
       "  'nothing',\n",
       "  'at',\n",
       "  'all',\n",
       "  '.'],\n",
       " ['The',\n",
       "  'two',\n",
       "  'most',\n",
       "  'important',\n",
       "  'days',\n",
       "  'in',\n",
       "  'your',\n",
       "  'life',\n",
       "  'are',\n",
       "  'the',\n",
       "  'day',\n",
       "  'you',\n",
       "  'are',\n",
       "  'born',\n",
       "  'and',\n",
       "  'the',\n",
       "  'day',\n",
       "  'you',\n",
       "  'find',\n",
       "  'out',\n",
       "  'why',\n",
       "  '.']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 각 문자을 단어토근화 해 봅시다. \n",
    "words = []\n",
    "sent = sent_tokenize(text_sample)\n",
    "for i in sent:\n",
    "    words.append(word_tokenize(i))\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83581a3b-9aa5-4a0d-8522-3ecf0e7c67d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/sonsoohan/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/sonsoohan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9654143a-3724-4a19-b19f-b11847ef3042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swim swim swim\n"
     ]
    }
   ],
   "source": [
    "lemma = WordNetLemmatizer()\n",
    "print(lemma.lemmatize('swimming','v'),lemma.lemmatize('swam','v'),lemma.lemmatize('swum','v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0b5097-9732-4307-a12a-0328ba0670ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk stemmer 종류 3가지 \n",
    "poter, lancaster, snowball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41691f0-30f6-4f9b-9b5b-c5ba29d29f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 피처 벡터화란?\n",
    "텍스트에서 단어를 추출해 피쳐로 할당하고 단어의 빈도를 값으로 벡터화 하는것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6dab6e-0c4e-48bf-8867-946db9c903c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 피처 벡터화의 두가지 방법은?\n",
    "BoW, Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6238361f-0be4-465c-8d5a-35a2dc6e6e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 분석의 프로세스를 1,2,3 단게로 서술하시오\n",
    "1. 텍스트 사전 준비작업(전처리)\n",
    "2. 피처 벡터화/추출\n",
    "3. ML 모델 수립 및 학습/예측/평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a1f5396-fc65-47c7-a4ee-db68a5884da2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'From: egreen@east.sun.com (Ed Green - Pixel Cruncher)\\nSubject: Re: Observation re: helmets\\nOrganization: Sun Microsystems, RTP, NC\\nLines: 21\\nDistribution: world\\nReply-To: egreen@east.sun.com\\nNNTP-Posting-Host: laser.east.sun.com\\n\\nIn article 211353@mavenry.altcit.eskimo.com, maven@mavenry.altcit.eskimo.com (Norman Hamer) writes:\\n> \\n> The question for the day is re: passenger helmets, if you don\\'t know for \\n>certain who\\'s gonna ride with you (like say you meet them at a .... church \\n>meeting, yeah, that\\'s the ticket)... What are some guidelines? Should I just \\n>pick up another shoei in my size to have a backup helmet (XL), or should I \\n>maybe get an inexpensive one of a smaller size to accomodate my likely \\n>passenger? \\n\\nIf your primary concern is protecting the passenger in the event of a\\ncrash, have him or her fitted for a helmet that is their size.  If your\\nprimary concern is complying with stupid helmet laws, carry a real big\\nspare (you can put a big or small head in a big helmet, but not in a\\nsmall one).\\n\\n---\\nEd Green, former Ninjaite |I was drinking last night with a biker,\\n  Ed.Green@East.Sun.COM   |and I showed him a picture of you.  I said,\\nDoD #0111  (919)460-8302  |\"Go on, get to know her, you\\'ll like her!\"\\n (The Grateful Dead) -->  |It seemed like the least I could do...\\n\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "news = fetch_20newsgroups(subset=\"all\",random_state=156) \n",
    "news.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b8fb1319-1df6-45fd-9e72-9c12df69ec41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# news.pkl 파일로 저장하기\n",
    "import pickle   # 방법1\n",
    "with open('news.pkl', 'wb') as f:\n",
    "    pickle.dump(news, f) \n",
    "with open('news.pkl', 'rb') as f:\n",
    "    data = pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e71d56bb-2488-4d4e-b579-fa58c752422d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'From: egreen@east.sun.com (Ed Green - Pixel Cruncher)\\nSubject: Re: Observation re: helmets\\nOrganization: Sun Microsystems, RTP, NC\\nLines: 21\\nDistribution: world\\nReply-To: egreen@east.sun.com\\nNNTP-Posting-Host: laser.east.sun.com\\n\\nIn article 211353@mavenry.altcit.eskimo.com, maven@mavenry.altcit.eskimo.com (Norman Hamer) writes:\\n> \\n> The question for the day is re: passenger helmets, if you don\\'t know for \\n>certain who\\'s gonna ride with you (like say you meet them at a .... church \\n>meeting, yeah, that\\'s the ticket)... What are some guidelines? Should I just \\n>pick up another shoei in my size to have a backup helmet (XL), or should I \\n>maybe get an inexpensive one of a smaller size to accomodate my likely \\n>passenger? \\n\\nIf your primary concern is protecting the passenger in the event of a\\ncrash, have him or her fitted for a helmet that is their size.  If your\\nprimary concern is complying with stupid helmet laws, carry a real big\\nspare (you can put a big or small head in a big helmet, but not in a\\nsmall one).\\n\\n---\\nEd Green, former Ninjaite |I was drinking last night with a biker,\\n  Ed.Green@East.Sun.COM   |and I showed him a picture of you.  I said,\\nDoD #0111  (919)460-8302  |\"Go on, get to know her, you\\'ll like her!\"\\n (The Grateful Dead) -->  |It seemed like the least I could do...\\n\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickle.dump(news, open('news.pkl','wb'))   # 방법2\n",
    "data = pickle.load(open('news.pkl','rb'))\n",
    "data.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b474d3-5830-46ea-b341-b2e01619cc2b",
   "metadata": {},
   "source": [
    "##### 2교시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b0131dd2-27bc-4028-9369-6043f8767e13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alt.atheism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>comp.os.ms-windows.misc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          0\n",
       "0               alt.atheism\n",
       "1             comp.graphics\n",
       "2   comp.os.ms-windows.misc\n",
       "3  comp.sys.ibm.pc.hardware"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "news.target\n",
    "pd.DataFrame(news.target_names)[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "306f1814-bf92-4fe9-83aa-164179cdf8af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4, 15, 10, ...,  3,  9,  9])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = fetch_20newsgroups(subset='train',remove=('headers','footers','quotes'),random_state=156)\n",
    "X_train = train.data\n",
    "y_train = train.target\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "056ba451-ed6d-4655-a64c-b18482db42f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4, 11,  1, ...,  6, 17, 15])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = fetch_20newsgroups(subset='test',remove=('headers','footers','quotes'),random_state=156)\n",
    "X_test = test.data\n",
    "y_test = test.target\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "91cea305-7133-4422-b8bb-227cef02e43e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<11314x101631 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1103627 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cnt_vect = CountVectorizer()\n",
    "cnt_vect.fit(X_train)\n",
    "X_train_cnt_vect = cnt_vect.transform(X_train)\n",
    "X_train_cnt_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "521361ac-4f21-4b0c-b960-235d2d3d6d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터 Text의 CountVectorizer Shape: (11314, 101631)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Count Vectorization으로 feature extraction 변환 수행. \n",
    "cnt_vect = CountVectorizer()\n",
    "\n",
    "cnt_vect.fit(X_train)\n",
    "X_train_cnt_vect = cnt_vect.transform(X_train)\n",
    "\n",
    "# 학습 데이터로 fit( )된 CountVectorizer를 이용하여 테스트 데이터를 feature extraction 변환 수행. \n",
    "X_test_cnt_vect = cnt_vect.transform(X_test)\n",
    "\n",
    "print('학습 데이터 Text의 CountVectorizer Shape:',X_train_cnt_vect.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1ecc5bfa-dd90-4a5a-a26a-21956d803125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorized Logistic Regression 의 예측 정확도는 0.616\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# LogisticRegression을 이용하여 학습/예측/평가 수행. \n",
    "lr_clf = LogisticRegression(solver='liblinear')\n",
    "lr_clf.fit(X_train_cnt_vect , y_train)\n",
    "pred = lr_clf.predict(X_test_cnt_vect)\n",
    "print('CountVectorized Logistic Regression 의 예측 정확도는 {0:.3f}'.format(accuracy_score(y_test,pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "15802b6b-8cdd-4531-8795-5a25b01d32af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Logistic Regression 의 예측 정확도는 0.678\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# TF-IDF Vectorization 적용하여 학습 데이터셋과 테스트 데이터 셋 변환. \n",
    "tfidf_vect = TfidfVectorizer()\n",
    "tfidf_vect.fit(X_train)\n",
    "X_train_tfidf_vect = tfidf_vect.transform(X_train)\n",
    "X_test_tfidf_vect = tfidf_vect.transform(X_test)\n",
    "\n",
    "# LogisticRegression을 이용하여 학습/예측/평가 수행. \n",
    "lr_clf = LogisticRegression(solver='liblinear')\n",
    "lr_clf.fit(X_train_tfidf_vect , y_train)\n",
    "pred = lr_clf.predict(X_test_tfidf_vect)\n",
    "print('TF-IDF Logistic Regression 의 예측 정확도는 {0:.3f}'.format(accuracy_score(y_test ,pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f0ad60-cbee-4472-81fa-ab748e139fd2",
   "metadata": {},
   "source": [
    "##### 3교시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d1468672-c34c-4e75-b8a8-b226ac944b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Vectorized Logistic Regression 의 예측 정확도는 0.690\n"
     ]
    }
   ],
   "source": [
    "# stop words 필터링을 추가하고 ngram을 기본(1,1)에서 (1,2)로 변경하여 Feature Vectorization 적용.\n",
    "tfidf_vect = TfidfVectorizer(stop_words='english', ngram_range=(1,2), max_df=300 )\n",
    "tfidf_vect.fit(X_train)\n",
    "X_train_tfidf_vect = tfidf_vect.transform(X_train)\n",
    "X_test_tfidf_vect = tfidf_vect.transform(X_test)\n",
    "\n",
    "lr_clf = LogisticRegression(solver='liblinear')\n",
    "lr_clf.fit(X_train_tfidf_vect , y_train)\n",
    "pred = lr_clf.predict(X_test_tfidf_vect)\n",
    "print('TF-IDF Vectorized Logistic Regression 의 예측 정확도는 {0:.3f}'.format(accuracy_score(y_test ,pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e8de46-d544-4bb4-a123-32861a270874",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 최적 C 값 도출 튜닝 수행. CV는 3 Fold셋으로 설정. \n",
    "params = { 'C':[0.01, 0.1, 1, 5, 10]}\n",
    "grid_cv_lr = GridSearchCV(lr_clf ,param_grid=params , cv=3 , scoring='accuracy' , verbose=1 )\n",
    "grid_cv_lr.fit(X_train_tfidf_vect , y_train)\n",
    "print('Logistic Regression best C parameter :',grid_cv_lr.best_params_ )\n",
    "\n",
    "# 최적 C 값으로 학습된 grid_cv로 예측 수행하고 정확도 평가. \n",
    "pred = grid_cv_lr.predict(X_test_tfidf_vect)\n",
    "print('TF-IDF Vectorized Logistic Regression 의 예측 정확도는 {0:.3f}'.format(accuracy_score(y_test ,pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "127038bc-775b-4453-b2f2-9857cd447ffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 10}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_cv_lr.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5257f968-e7ad-4418-8b8b-f0e5048bc322",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# TfidfVectorizer 객체를 tfidf_vect 객체명으로, LogisticRegression객체를 lr_clf 객체명으로 생성하는 Pipeline생성\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf_vect', TfidfVectorizer(stop_words='english', ngram_range=(1,2), max_df=300)),\n",
    "    ('lr_clf', LogisticRegression(solver='liblinear', C=10))\n",
    "])\n",
    "\n",
    "# 별도의 TfidfVectorizer객체의 fit_transform( )과 LogisticRegression의 fit(), predict( )가 필요 없음. \n",
    "# pipeline의 fit( ) 과 predict( ) 만으로 한꺼번에 Feature Vectorization과 ML 학습/예측이 가능. \n",
    "pipeline.fit(X_train, y_train)\n",
    "pred = pipeline.predict(X_test)\n",
    "print('Pipeline을 통한 Logistic Regression 의 예측 정확도는 {0:.3f}'.format(accuracy_score(y_test ,pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545625c4-45d1-4d0f-b662-820e13d4bc76",
   "metadata": {},
   "source": [
    "##### 5교시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0b52002f-1c1c-40b5-801b-d13ed56d8124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 감성분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d6212c8d-1f20-46c1-8ce8-a834cceb2bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede2a70e-2e81-4b87-a34d-cfa79402ac2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sample = 'Turn your wounds into wisdom.\\\n",
    "               Life is either a daring adventure or nothing at all.\\\n",
    "               The two most important days in your life are the day you are born and the day you find out why.' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "295ea748-110e-4151-be88-42a737187d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/sonsoohan/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4723a539-a356-46ea-b25a-4b3f4978482f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.751, 'pos': 0.249, 'compound': 0.8516}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senti_analyser = SentimentIntensityAnalyzer()\n",
    "senti_scores = senti_analyser.polarity_scores(text_sample)\n",
    "senti_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cb36bd37-8b6b-4160-b47d-07733b24139b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8516"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senti_scores['compound']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c37363-6224-421b-a81e-624270cd50a0",
   "metadata": {},
   "source": [
    "##### 6교시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4ce43682-8aa3-42aa-94f3-8878be34e205",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cos_similarity(v1, v2):\n",
    "    dot_product = np.dot(v1, v2)\n",
    "    l2_norm = (np.sqrt(sum(np.square(v1))) * np.sqrt(sum(np.square(v2))))\n",
    "    similarity = dot_product / l2_norm     \n",
    "    \n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "87be3cc4-c2af-48dc-8cf6-0d658ad87163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 18)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "doc_list = ['if you take the blue pill, the story ends' ,\n",
    "            'if you take the red pill, you stay in Wonderland',\n",
    "            'if you take the red pill, I show you how deep the rabbit hole goes']\n",
    "\n",
    "tfidf_vect_simple = TfidfVectorizer()\n",
    "feature_vect_simple = tfidf_vect_simple.fit_transform(doc_list)\n",
    "print(feature_vect_simple.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "240d957d-2a29-45f4-8348-c76664fc5b4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.2.1'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecfa4c5-bc06-4128-a53d-8076097f18ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
